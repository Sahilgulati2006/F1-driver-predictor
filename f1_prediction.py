# -*- coding: utf-8 -*-
"""f1_prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/115g4M5hnVuMKkUOD-ycjbdB2AER-GWOY
"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

from google.colab import drive
drive.mount('/content/drive')

!kaggle datasets download -d rohanrao/formula-1-world-championship-1950-2020

from zipfile import ZipFile
file_name = '/content/formula-1-world-championship-1950-2020.zip'
with ZipFile(file_name, 'r') as zip:
    zip.extractall()
    print('file extracted')

!ls

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from imblearn.over_sampling import SMOTE
from sklearn.metrics import f1_score

# Load data
extracted_folder = '/content/'
results_df = pd.read_csv(os.path.join(extracted_folder, 'results.csv'))
races_df = pd.read_csv(os.path.join(extracted_folder, 'races.csv'))
drivers_df = pd.read_csv(os.path.join(extracted_folder, 'drivers.csv'))
constructors_df = pd.read_csv(os.path.join(extracted_folder, 'constructors.csv'))

# Merge results with races for context
merged_df = pd.merge(results_df, races_df, on='raceId', how='left')

# Encode categorical features
driver_encoder = LabelEncoder()
constructor_encoder = LabelEncoder()
merged_df['driver_encoded'] = driver_encoder.fit_transform(merged_df['driverId'])
merged_df['constructor_encoded'] = constructor_encoder.fit_transform(merged_df['constructorId'])

# Rare drivers: Assign special value
rare_driver_threshold = 10
rare_drivers = merged_df['driver_encoded'].value_counts()[merged_df['driver_encoded'].value_counts() < rare_driver_threshold].index
merged_df.loc[merged_df['driver_encoded'].isin(rare_drivers), 'driver_encoded'] = -1

# Feature engineering
merged_df['driver_constructor_points'] = merged_df.groupby(['driver_encoded', 'constructor_encoded'])['points'].transform('mean')
merged_df['rolling_avg_points'] = merged_df.groupby('driver_encoded')['points'].transform(lambda x: x.rolling(window=5, min_periods=1).mean())
merged_df['grid_start_vs_final_position'] = merged_df['grid'] - merged_df['positionOrder']

# Select features and target
features = [
    'grid', 'laps', 'driver_constructor_points', 'rolling_avg_points',
    'constructor_encoded', 'year', 'round', 'fastestLapTime', 'grid_start_vs_final_position'
]

def time_to_seconds(time_str):
    if pd.isnull(time_str):
        return np.nan
    if isinstance(time_str, (int, float)):
        return time_str
    if isinstance(time_str, str):
        parts = time_str.split(':')
        if len(parts) == 2:
            minutes, seconds_milliseconds = parts
            seconds, milliseconds = seconds_milliseconds.split('.')
            total_seconds = int(minutes) * 60 + int(seconds) + int(milliseconds) / 1000
            return total_seconds
        else:
            return np.nan
    else:
        return np.nan

X = merged_df[features]
y = merged_df['driver_encoded']

X['fastestLapTime'] = X['fastestLapTime'].apply(time_to_seconds)

# Handle missing values
X.fillna(0, inplace=True)

# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Encode target variable as categorical
y_encoded = to_categorical(y)

# Oversample using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, np.argmax(y_encoded, axis=1))

# Convert target back to categorical after resampling
y_resampled_categorical = to_categorical(y_resampled)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled_categorical, test_size=0.2, random_state=42)

# Compute class weights
class_weights = compute_class_weight('balanced', classes=np.unique(y_resampled), y=y_resampled)
class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}

# Define model
model = Sequential([
    Dense(512, input_dim=X_train.shape[1], activation='relu'),
    BatchNormalization(),
    Dropout(0.4),
    Dense(256, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Dense(128, activation='relu'),
    BatchNormalization(),
    Dense(y_resampled_categorical.shape[1], activation='softmax')
])

# Compile model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Callbacks for early stopping and learning rate adjustment
early_stopping = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-5)

# Train the model
history = model.fit(
    X_train, y_train,
    epochs=35,
    batch_size=64,
    validation_split=0.2,
    class_weight=class_weights_dict,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# Make predictions
y_pred_probs = model.predict(X_test)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = np.argmax(y_test, axis=1)

# Calculate F1 Score
f1 = f1_score(y_true, y_pred, average='weighted')
print(f"Test F1 Score: {f1:.4f}")


# Predict on test data
predictions = np.argmax(model.predict(X_test), axis=1)
predicted_drivers = driver_encoder.inverse_transform(predictions)

# Display 10 sample predictions
sample_predictions = pd.DataFrame({
    'Actual Driver': driver_encoder.inverse_transform(np.argmax(y_test, axis=1)),
    'Predicted Driver': predicted_drivers
})

print(sample_predictions.head(10))

# Save the trained model
model.save('f1_driver_predictor.keras', save_format='keras')